{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWTwpVcAhEQl"
      },
      "source": [
        "# **AI Chatbots f√ºr AO fracture classification**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTvJntMww5Ur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67cbd8db-7019-4a46-f872-2a4f70893f30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for llama-index (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index==0.5.0 -q\n",
        "!pip install gradio -q\n",
        "!pip install PyPDF2 -q\n",
        "!pip install --upgrade --no-cache-dir gdown -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "16IjpR3DVIYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import gradio as gr\n",
        "import openai\n",
        "from llama_index import (\n",
        "    GPTSimpleVectorIndex,\n",
        "    LLMPredictor,\n",
        "    SimpleDirectoryReader,\n",
        "    ServiceContext,\n",
        ")\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "\n",
        "# Set the OpenAI API key. If unsure about obtaining the API key, refer to https://platform.openai.com/account/api-keys for more information.\n",
        "# For estimating costs associated with index creation and chatbot usage, please visit: https://openai.com/pricing\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'sk-ENTER_YOUR_API_CODE'\n",
        "\n",
        "\n",
        "# Set the folder path for AO pdf data and the rebuild index flag. This script doesn't include\n",
        "# importing AO/OTA Classification or a predefined index due to potential license restrictions on AO guidelines.\n",
        "# For more information, refer to the AO website.\n",
        "\n",
        "\n",
        "\n",
        "FOLDER_PATH = \"AO\"\n",
        "REBUILD_INDEX = True\n",
        "\n",
        "\n",
        "\n",
        "def setup_index(folder_path, rebuild_index):\n",
        "    if not os.path.exists(FOLDER_PATH):\n",
        "        os.mkdir(FOLDER_PATH)\n",
        "    index_file_name = 'AOindex.json'\n",
        "\n",
        "    # Check if the folder path exists and contains PDF files\n",
        "    if not os.path.exists(folder_path) or not glob.glob(f\"{folder_path}/*.pdf\"):\n",
        "        print(\"The folder path either does not exist or does not contain PDF files.\")\n",
        "        return\n",
        "\n",
        "    # Rebuild the index if rebuild_index is True or the index file does not exist\n",
        "    if rebuild_index or not os.path.isfile(index_file_name):\n",
        "        # Load documents from the folder path\n",
        "        documents = SimpleDirectoryReader(folder_path).load_data()\n",
        "\n",
        "        # Initialize the LLMPredictor with the ChatOpenAI model\n",
        "        llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", max_tokens=512, request_timeout=120))\n",
        "\n",
        "        # Create a service context for the LLMPredictor\n",
        "        service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size_limit=512)\n",
        "\n",
        "        # Create an index from the documents and save it to disk\n",
        "        index = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)\n",
        "        index.save_to_disk(index_file_name)\n",
        "\n",
        "def get_combined_chatbot_response(input_text):\n",
        "    # Format the input text\n",
        "    input_text = f'Case: {input_text} Task: Analyze the provided fracture description, determine the AO Classification based on the details of the affected bone, location, and characteristics, and provide as response !only! the appropriate AO Code!'\n",
        "\n",
        "    # Initialize the LLMPredictor with the ChatOpenAI model used in FracChat\n",
        "    llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", max_tokens=512, request_timeout=120))\n",
        "\n",
        "    # Create a service context for the LLMPredictor\n",
        "    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size_limit=512)\n",
        "\n",
        "    # Initialize the LLMPredictor with the ChatOpenAI model used in FracChat4\n",
        "    llm_predictor4 = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\", max_tokens=512, request_timeout=120))\n",
        "\n",
        "    # Create a service context for the LLMPredictor4\n",
        "    service_context4 = ServiceContext.from_defaults(llm_predictor=llm_predictor4, chunk_size_limit=512)\n",
        "\n",
        "    # Load the index from disk\n",
        "    index = GPTSimpleVectorIndex.load_from_disk('AOindex_simple35.json')\n",
        "\n",
        "    # Define the messages to be sent to the models without index\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are ChatGPT, a large language model trained by OpenAI. Answer as concisely as possible.\"},\n",
        "        {\"role\": \"user\", \"content\": input_text},\n",
        "    ]\n",
        "\n",
        "    # FracChat3.5 approach: Query the index and get the response from Top 3 Text nodes using GPT-3.5-Turbo\n",
        "    response = index.query(input_text, response_mode=\"compact\", service_context=service_context, similarity_top_k=3)\n",
        "    output_FracChat = response.response.replace('\\n', '\\\\n')\n",
        "\n",
        "    # FracChat4 approach: Query the index and get the response from Top 3 Text nodes using GPT-4\n",
        "    response4 = index.query(input_text, response_mode=\"compact\", service_context=service_context4, similarity_top_k=3)\n",
        "    output_FracChat4 = response4.response.replace('\\n', '\\\\n')\n",
        "\n",
        "    # Get the response from GPT-3.5-Turbo\n",
        "    response35 = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=messages, timeout=120)\n",
        "    output35 = response35.choices[0].message['content'].replace('\\n', '\\\\n')\n",
        "\n",
        "    # Get the response from GPT-4\n",
        "    response4 = openai.ChatCompletion.create(model=\"gpt-4\", messages=messages, timeout=120)\n",
        "    output4 = response4.choices[0].message['content'].replace('\\n', '\\\\n')\n",
        "\n",
        "    # Combine the responses from all models\n",
        "    answer = f\"FracChat using GPT4: {output_FracChat4}\\n\\n FracChat using GPT3.5-turbo: {output_FracChat}\\n\\nGPT 3.5-Turbo: {output35}\\n\\nGPT 4: {output4}\"\n",
        "\n",
        "    return answer\n",
        "\n",
        "\n",
        "def launch_interface(chatbot_function):\n",
        "    iface = gr.Interface(\n",
        "        fn=get_combined_chatbot_response,\n",
        "        inputs=[gr.Textbox(lines=15, label=\"Enter your case\")],\n",
        "        outputs=gr.Textbox(lines=15, label=\"Imaging Recommendations\"),\n",
        "        title=\"AI Chatbots for AO fracture classification\"\n",
        "    )\n",
        "\n",
        "    # Launch the interface\n",
        "    iface.launch(share=True, debug=True)\n",
        "\n",
        "def main():\n",
        "    setup_index(FOLDER_PATH, REBUILD_INDEX)\n",
        "    launch_interface(get_combined_chatbot_response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "kFaXpZjieSV_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "c2f498cb-9a51-4080-f4e7-84d519350a6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dnbboXJ-7WJ5dX31dZ-_4fIHmYYhUdXX\n",
            "To: /content/AOindex_simple35.json\n",
            "100% 16.3M/16.3M [00:00<00:00, 25.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wH-mZnycAjYqBoVRdGQHRG-k44MrDtX7\n",
            "To: /content/AO/AO.pdf\n",
            "100% 19.1M/19.1M [00:00<00:00, 181MB/s]\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://8745ca3b337661e1aa.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8745ca3b337661e1aa.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:llama_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://8745ca3b337661e1aa.gradio.live\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}